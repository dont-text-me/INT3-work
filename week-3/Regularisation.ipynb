{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKXXs11YcqHM"
   },
   "source": [
    "# Regularisation: Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSzLH8OXewlE"
   },
   "source": [
    "**Introduction**\n",
    "\n",
    "Imagine a data set which includes two input variables, X<sub>i</sub> and X<sub>j</sub>, X<sub>j</sub>=-X<sub>i</sub>, such that they have no genuine bearing on the output y. Still, a linear model that sets β<sub>j</sub>=-β<sub>i</sub> will have the same accuracy as the one from which X<sub>i</sub> and X<sub>j</sub> are removed, as  the two terms in the sum β<sub>i</sub>X<sub>i</sub>+β<sub>j</sub>X<sub>j</sub> would cancel out. This situation can be mitigated if the loss function of our linear regression, the ordinary least squares (OLS), is extended with an additional *penalty* term, which pushes down the parameters  β<sub>1</sub>... β<sub>p</sub>. Everything else being equal, this would allow us to reduce the parameters β<sub>i</sub> and β<sub>j</sub> of our example to as close to zero as possible without any loss of accuracy. \n",
    "\n",
    "When the penalty term is proportional to L2 = Σ(β<sub>i</sub><sup>2</sup>), the resulting regression is known as [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn-linear-model-ridge) regression (with a loss function OLS+λ.L2), while using penalty term L1 = Σ|β<sub>i</sub>| (loss function OLS+λ.L1) corresponds to the so-called [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)  regression. \n",
    "\n",
    "L1 and L2 are also known as *regularisation* terms. λ is referred to as the complexity parameter. You can also see the letter α (alpha) used instead of λ - this includes the entire scikit-learn documentation. \n",
    "\n",
    "Using regularisation terms - L1, L2 or their combination (with the result known as [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html?highlight=elastic%20net#sklearn-linear-model-elasticnet)) has as an effect a reduction of the variance of the model parameters β<sub>1</sub>... β<sub>p</sub> as we vary the training data. The price to pay is a larger model bias, i.e. the average (squared) error we end up with as we vary our training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2ozqzDKYbmj"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "In this exercise you will use both Ridge and Lasso regression, and also plain Linear Regression to build regression models for the California house prices dataset. The task for this dataset is to learn a model that predicts the median price of a house (in a California district) from 8 variables describing that district.\n",
    "\n",
    "To choose the correct complexity penalty for Ridge and Lasso regression you should use the built-in *cross-validation* (see online lecture) that is available in scikit-learn via the classes [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html?highlight=ridgecv#sklearn.linear_model.RidgeCV) and [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). Look up how to use them in the scikit-learn documentation.\n",
    "\n",
    "To help you out here is the initial part of my Python program for doing this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T11:57:39.254228Z",
     "start_time": "2024-02-28T11:57:36.851936Z"
    },
    "id": "mrCqA9LtW2Zi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "(20640,)\n",
      "Index(['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
      "       'Latitude', 'Longitude'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "\n",
    "(cali, target) = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "print(cali.shape)\n",
    "print(target.shape)\n",
    "print(cali.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HT2LBLgZ2zu"
   },
   "source": [
    "**In your first experiment:**\n",
    "\n",
    "*   learn from the first 15,000 datapoints\n",
    "*   print out the learned parameter values for each predictor \n",
    "*   compute the R<sup>2</sup> score on the remainder. \n",
    "\n",
    "Do this for linear regression, ridge regression and lasso regression, then: \n",
    "\n",
    "*   For ridge and lasso regression print out the complexity penalty value cross-validation found.\n",
    "*   Inspect the learned parameter values for each predictor, and comment on their significance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T11:57:39.259124Z",
     "start_time": "2024-02-28T11:57:39.254472Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_large_train, X_large_test = cali.head(15000), cali[15000:]\n",
    "y_large_train, y_large_test = target.head(15000), target[15000:]\n",
    "X_small_train, X_small_test = cali.head(150), cali[150:]\n",
    "y_small_train, y_small_test = target.head(150), target[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T11:57:39.658904Z",
     "start_time": "2024-02-28T11:57:39.257102Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "\n",
    "linear_model = LinearRegression(fit_intercept=True)\n",
    "lasso_model = LassoCV(fit_intercept=True)\n",
    "ridge_model = RidgeCV(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T12:33:44.771771Z",
     "start_time": "2024-02-28T12:33:44.059765Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear parameters: [ 0.44313  0.00689 -0.10623  0.6213  -0.00001 -0.00782 -0.38536 -0.36781], intercept: -30.192307069444364\n",
      "Ridge parameters: [ 0.44181  0.00691 -0.1038   0.60871 -0.00001 -0.00782 -0.38514 -0.36731], intercept: -30.13432926986759\n",
      "Lasso parameters: [ 0.38459  0.00867  0.00428  0.      -0.00001 -0.00766 -0.30724 -0.26938], intercept: -20.973184812801076\n"
     ]
    }
   ],
   "source": [
    "linear_model.fit(X_large_train, y_large_train)\n",
    "ridge_model.fit(X_large_train, y_large_train)\n",
    "lasso_model.fit(X_large_train, y_large_train)\n",
    "with np.printoptions(suppress=True, precision=5):\n",
    "    print(\n",
    "        f\"Linear parameters: {linear_model.coef_}, intercept: {linear_model.intercept_}\"\n",
    "    )\n",
    "    print(f\"Ridge parameters: {ridge_model.coef_}, intercept: {ridge_model.intercept_}\")\n",
    "    print(f\"Lasso parameters: {lasso_model.coef_}, intercept: {lasso_model.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T11:57:40.313704Z",
     "start_time": "2024-02-28T11:57:40.127613Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear r2 score: 0.5931322421964107\n",
      "Ridge r2 score: 0.5928319031838527\n",
      "Lasso r2 score: 0.5514968914697881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(f\"Linear r2 score: {r2_score(y_large_test, linear_model.predict(X_large_test))}\")\n",
    "print(f\"Ridge r2 score: {r2_score(y_large_test, ridge_model.predict(X_large_test))}\")\n",
    "print(f\"Lasso r2 score: {r2_score(y_large_test, lasso_model.predict(X_large_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T12:33:09.612865Z",
     "start_time": "2024-02-28T12:33:09.463199Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear parameters (first 150): [ 0.22253  0.00261 -0.08488 -0.49951  0.00008  0.0265   2.05076 16.96091], intercept: 1998.0327743138705\n",
      "Ridge parameters (first 150): [ 0.37075  0.00394 -0.10873 -0.88108  0.00014 -0.01342 -0.38323  3.80683], intercept: 481.971647275177, alpha: 0.1\n",
      "Lasso parameters (first 150): [ 0.35888 -0.      -0.01961 -0.       0.00022 -0.      -0.       0.     ], intercept: 0.7851621727478544, alpha: 0.100946851336\n",
      "Linear r2 score (first 150): -1961.0500902972406\n",
      "Ridge r2 score (first 150): -148.73611465686602\n",
      "Lasso r2 score (first 150): 0.33447062483807477\n"
     ]
    }
   ],
   "source": [
    "linear_model = LinearRegression(fit_intercept=True)\n",
    "lasso_model = LassoCV(fit_intercept=True)\n",
    "ridge_model = RidgeCV(fit_intercept=True)\n",
    "linear_model.fit(X_small_train, y_small_train)\n",
    "ridge_model.fit(X_small_train, y_small_train)\n",
    "lasso_model.fit(X_small_train, y_small_train)\n",
    "with np.printoptions(suppress=True, precision=5):\n",
    "    print(\n",
    "        f\"Linear parameters (first 150): {linear_model.coef_}, intercept: {linear_model.intercept_}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Ridge parameters (first 150): {ridge_model.coef_}, intercept: {ridge_model.intercept_}, alpha: {ridge_model.alpha_}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Lasso parameters (first 150): {lasso_model.coef_}, intercept: {lasso_model.intercept_}, alpha: {lasso_model.alpha_}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Linear r2 score (first 150): {r2_score(y_small_test, linear_model.predict(X_small_test))}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Ridge r2 score (first 150): {r2_score(y_small_test, ridge_model.predict(X_small_test))}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Lasso r2 score (first 150): {r2_score(y_small_test, lasso_model.predict(X_small_test))}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7uEQm0DcvqO"
   },
   "source": [
    "## L1 and L2 regularisation with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMdh4fj1iQa1"
   },
   "source": [
    "So, Lasso regression is simply a regression model that uses L1 regularisation;  a model that uses L2 regularisation is called Ridge regression. This means that we can easily modify our implementation of Linear regression by tweaking the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T11:58:51.702707Z",
     "start_time": "2024-02-28T11:58:51.689559Z"
    },
    "id": "YolUjUfCXufP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, fit loss 50578.5859375, reg loss 0.5649606585502625\n",
      "epoch 1, fit loss 5699.0830078125, reg loss 0.5516319274902344\n",
      "epoch 2, fit loss 955.1901245117188, reg loss 0.5501037240028381\n",
      "epoch 3, fit loss 452.973388671875, reg loss 0.5498025417327881\n",
      "epoch 4, fit loss 399.03399658203125, reg loss 0.5496246814727783\n",
      "epoch 5, fit loss 392.4723205566406, reg loss 0.5494581460952759\n",
      "epoch 6, fit loss 390.919921875, reg loss 0.5492923259735107\n",
      "epoch 7, fit loss 389.89910888671875, reg loss 0.5491266846656799\n",
      "epoch 8, fit loss 388.9367370605469, reg loss 0.5489614605903625\n",
      "epoch 9, fit loss 387.98272705078125, reg loss 0.5487965941429138\n",
      "epoch 10, fit loss 387.0317077636719, reg loss 0.548632025718689\n",
      "epoch 11, fit loss 386.08331298828125, reg loss 0.5484679341316223\n",
      "epoch 12, fit loss 385.1372985839844, reg loss 0.5483042001724243\n",
      "epoch 13, fit loss 384.1937255859375, reg loss 0.5481408834457397\n",
      "epoch 14, fit loss 383.25262451171875, reg loss 0.5479779243469238\n",
      "epoch 15, fit loss 382.31390380859375, reg loss 0.5478153824806213\n",
      "epoch 16, fit loss 381.3775329589844, reg loss 0.5476531982421875\n",
      "epoch 17, fit loss 380.44366455078125, reg loss 0.5474914312362671\n",
      "epoch 18, fit loss 379.5120849609375, reg loss 0.5473300218582153\n",
      "epoch 19, fit loss 378.58294677734375, reg loss 0.5471689701080322\n",
      "epoch 20, fit loss 377.6561279296875, reg loss 0.5470083355903625\n",
      "epoch 21, fit loss 376.7317199707031, reg loss 0.5468480587005615\n",
      "epoch 22, fit loss 375.809814453125, reg loss 0.5466881990432739\n",
      "epoch 23, fit loss 374.89019775390625, reg loss 0.5465287566184998\n",
      "epoch 24, fit loss 373.97296142578125, reg loss 0.5463696718215942\n",
      "epoch 25, fit loss 373.05810546875, reg loss 0.5462109446525574\n",
      "epoch 26, fit loss 372.1455078125, reg loss 0.5460525751113892\n",
      "epoch 27, fit loss 371.2353210449219, reg loss 0.5458946228027344\n",
      "epoch 28, fit loss 370.3275451660156, reg loss 0.5457370281219482\n",
      "epoch 29, fit loss 369.4219970703125, reg loss 0.5455797910690308\n",
      "epoch 30, fit loss 368.518798828125, reg loss 0.5454229712486267\n",
      "epoch 31, fit loss 367.6178283691406, reg loss 0.5452665090560913\n",
      "epoch 32, fit loss 366.7192077636719, reg loss 0.5451103448867798\n",
      "epoch 33, fit loss 365.8228759765625, reg loss 0.5449545979499817\n",
      "epoch 34, fit loss 364.92889404296875, reg loss 0.544799268245697\n",
      "epoch 35, fit loss 364.0372314453125, reg loss 0.544644296169281\n",
      "epoch 36, fit loss 363.1477966308594, reg loss 0.5444896817207336\n",
      "epoch 37, fit loss 362.26068115234375, reg loss 0.5443353652954102\n",
      "epoch 38, fit loss 361.3758544921875, reg loss 0.5441814661026001\n",
      "epoch 39, fit loss 360.4932861328125, reg loss 0.5440279245376587\n",
      "epoch 40, fit loss 359.6129455566406, reg loss 0.5438748002052307\n",
      "epoch 41, fit loss 358.7348937988281, reg loss 0.5437219738960266\n",
      "epoch 42, fit loss 357.859130859375, reg loss 0.5435695648193359\n",
      "epoch 43, fit loss 356.98565673828125, reg loss 0.5434175133705139\n",
      "epoch 44, fit loss 356.11431884765625, reg loss 0.5432657599449158\n",
      "epoch 45, fit loss 355.2452697753906, reg loss 0.543114423751831\n",
      "epoch 46, fit loss 354.37847900390625, reg loss 0.5429633855819702\n",
      "epoch 47, fit loss 353.5138244628906, reg loss 0.5428128242492676\n",
      "epoch 48, fit loss 352.6514587402344, reg loss 0.542662501335144\n",
      "epoch 49, fit loss 351.7912902832031, reg loss 0.5425125360488892\n",
      "tensor(406.3126, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Function to compute the L1 loss of all weights in a model\n",
    "def L1regloss(model):\n",
    "    return sum([param.abs().sum() for param in model.parameters()])\n",
    "\n",
    "\n",
    "def L2regloss(model):\n",
    "    return sum([param.pow(2).sum() for param in model.parameters()])\n",
    "\n",
    "\n",
    "model = torch.nn.Linear(8, 1)\n",
    "X_train = torch.from_numpy(np.float32(cali[:15000]))\n",
    "y_train = torch.from_numpy(np.float32(target[:15000])).unsqueeze(1)\n",
    "X_test = torch.from_numpy(np.float32(cali[15000:]))\n",
    "y_test = torch.from_numpy(np.float32(target[15000:])).unsqueeze(1)\n",
    "epochs = 50\n",
    "\n",
    "# Setup optimiser\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-7)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# Main training loop\n",
    "for epoch in range(epochs):\n",
    "    y_predict = model(X_train)\n",
    "    fit_loss = criterion(y_train, y_predict)\n",
    "    # Use L1regloss for Lasso\n",
    "    reg_loss = L2regloss(model)\n",
    "    loss = fit_loss + reg_loss\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(\n",
    "        \"epoch {}, fit loss {}, reg loss {}\".format(\n",
    "            epoch, fit_loss.item(), reg_loss.item()\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Generalisation error\n",
    "print(criterion(model(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNLtKvAni9ZJ"
   },
   "source": [
    "**To do:** \n",
    "\n",
    "Modify the loss function in the above solution to use the L2 loss as penalty instead of L1 loss, thus obtaining an implementation of Ridge regression. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

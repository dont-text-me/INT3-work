{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKXXs11YcqHM"
   },
   "source": [
    "#Regularisation: Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSzLH8OXewlE"
   },
   "source": [
    "**Introduction**\n",
    "\n",
    "Imagine a data set which includes two input variables, X<sub>i</sub> and X<sub>j</sub>, X<sub>j</sub>=-X<sub>i</sub>, such that they have no genuine bearing on the output y. Still, a linear model that sets β<sub>j</sub>=-β<sub>i</sub> will have the same accuracy as the one from which X<sub>i</sub> and X<sub>j</sub> are removed, as  the two terms in the sum β<sub>i</sub>X<sub>i</sub>+β<sub>j</sub>X<sub>j</sub> would cancel out. This situation can be mitigated if the loss function of our linear regression, the ordinary least squares (OLS), is extended with an additional *penalty* term, which pushes down the parameters  β<sub>1</sub>... β<sub>p</sub>. Everything else being equal, this would allow us to reduce the parameters β<sub>i</sub> and β<sub>j</sub> of our example to as close to zero as possible without any loss of accuracy. \n",
    "\n",
    "When the penalty term is proportional to L2 = Σ(β<sub>i</sub><sup>2</sup>), the resulting regression is known as [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn-linear-model-ridge) regression (with a loss function OLS+λ.L2), while using penalty term L1 = Σ|β<sub>i</sub>| (loss function OLS+λ.L1) corresponds to the so-called [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)  regression. \n",
    "\n",
    "L1 and L2 are also known as *regularisation* terms. λ is referred to as the complexity parameter. You can also see the letter α (alpha) used instead of λ - this includes the entire scikit-learn documentation. \n",
    "\n",
    "Using regularisation terms - L1, L2 or their combination (with the result known as [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html?highlight=elastic%20net#sklearn-linear-model-elasticnet)) has as an effect a reduction of the variance of the model parameters β<sub>1</sub>... β<sub>p</sub> as we vary the training data. The price to pay is a larger model bias, i.e. the average (squared) error we end up with as we vary our training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2ozqzDKYbmj"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "In this exercise you will use both Ridge and Lasso regression, and also plain Linear Regression to build regression models for the California house prices dataset. The task for this dataset is to learn a model that predicts the median price of a house (in a California district) from 8 variables describing that district.\n",
    "\n",
    "To choose the correct complexity penalty for Ridge and Lasso regression you should use the built-in *cross-validation* (see online lecture) that is available in scikit-learn via the classes [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html?highlight=ridgecv#sklearn.linear_model.RidgeCV) and [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). Look up how to use them in the scikit-learn documentation.\n",
    "\n",
    "To help you out here is the initial part of my Python program for doing this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T12:33:40.150288Z",
     "start_time": "2024-02-24T12:33:40.133081Z"
    },
    "id": "mrCqA9LtW2Zi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "(20640,)\n",
      "Index(['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
      "       'Latitude', 'Longitude'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "\n",
    "(cali, target) = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "print(cali.shape)\n",
    "print(target.shape)\n",
    "print(cali.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HT2LBLgZ2zu"
   },
   "source": [
    "**In your first experiment:**\n",
    "\n",
    "*   learn from the first 15,000 datapoints\n",
    "*   print out the learned parameter values for each predictor \n",
    "*   compute the R<sup>2</sup> score on the remainder. \n",
    "\n",
    "Do this for linear regression, ridge regression and lasso regression, then: \n",
    "\n",
    "*   For ridge and lasso regression print out the complexity penalty value cross-validation found.\n",
    "*   Inspect the learned parameter values for each predictor, and comment on their significance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T12:57:16.431060Z",
     "start_time": "2024-02-24T12:57:16.424130Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_large_train, X_large_test = cali.head(15000), cali[15000:]\n",
    "y_large_train, y_large_test = target.head(15000), target[15000:]\n",
    "X_small_train, X_small_test = cali.head(150), cali[150:]\n",
    "y_small_train, y_small_test = target.head(150), target[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T12:57:18.498588Z",
     "start_time": "2024-02-24T12:57:18.487057Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "lasso_model = LassoCV()\n",
    "ridge_model = RidgeCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T12:57:22.360008Z",
     "start_time": "2024-02-24T12:57:21.835471Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear parameters: [ 0.44313377  0.00688834 -0.10622562  0.62129754 -0.00001225 -0.00781557\n",
      " -0.38535813 -0.36781462]\n",
      "Ridge parameters: [ 0.44181452  0.00690518 -0.10379775  0.60870933 -0.00001216 -0.00782168\n",
      " -0.38513899 -0.36730646]\n",
      "Lasso parameters: [ 0.38458748  0.00866872  0.00428265  0.         -0.00000633 -0.00766105\n",
      " -0.30723866 -0.26938089]\n"
     ]
    }
   ],
   "source": [
    "linear_model.fit(X_large_train, y_large_train)\n",
    "ridge_model.fit(X_large_train, y_large_train)\n",
    "lasso_model.fit(X_large_train, y_large_train)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(f\"Linear parameters: {linear_model.coef_}\")\n",
    "print(f\"Ridge parameters: {ridge_model.coef_}\")\n",
    "print(f\"Lasso parameters: {lasso_model.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T12:57:25.240073Z",
     "start_time": "2024-02-24T12:57:25.147725Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear r2 score: 0.5931322421964107\n",
      "Ridge r2 score: 0.5928319031838527\n",
      "Lasso r2 score: 0.5514968914697881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(f\"Linear r2 score: {r2_score(y_large_test, linear_model.predict(X_large_test))}\")\n",
    "print(f\"Ridge r2 score: {r2_score(y_large_test, ridge_model.predict(X_large_test))}\")\n",
    "print(f\"Lasso r2 score: {r2_score(y_large_test, lasso_model.predict(X_large_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T12:59:17.913032Z",
     "start_time": "2024-02-24T12:59:17.818321Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear parameters (first 150): [ 0.22252695  0.00261472 -0.08487863 -0.49951197  0.00007863  0.02650346\n",
      "  2.05076257 16.96090827]\n",
      "Ridge parameters (first 150): [ 0.37075067  0.00394285 -0.10873192 -0.8810758   0.00014321 -0.01341508\n",
      " -0.38322814  3.8068281 ]\n",
      "Lasso parameters (first 150): [ 0.35888259 -0.         -0.01960501 -0.          0.00021657 -0.\n",
      " -0.          0.        ]\n",
      "Linear r2 score (first 150): -1961.0500902972406\n",
      "Ridge r2 score (first 150): -148.73611465686602\n",
      "Lasso r2 score (first 150): 0.33447062483807477\n"
     ]
    }
   ],
   "source": [
    "linear_model = LinearRegression()\n",
    "lasso_model = LassoCV()\n",
    "ridge_model = RidgeCV()\n",
    "linear_model.fit(X_small_train, y_small_train)\n",
    "ridge_model.fit(X_small_train, y_small_train)\n",
    "lasso_model.fit(X_small_train, y_small_train)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(f\"Linear parameters (first 150): {linear_model.coef_}\")\n",
    "print(f\"Ridge parameters (first 150): {ridge_model.coef_}\")\n",
    "print(f\"Lasso parameters (first 150): {lasso_model.coef_}\")\n",
    "print(\n",
    "    f\"Linear r2 score (first 150): {r2_score(y_small_test, linear_model.predict(X_small_test))}\"\n",
    ")\n",
    "print(\n",
    "    f\"Ridge r2 score (first 150): {r2_score(y_small_test, ridge_model.predict(X_small_test))}\"\n",
    ")\n",
    "print(\n",
    "    f\"Lasso r2 score (first 150): {r2_score(y_small_test, lasso_model.predict(X_small_test))}\"\n",
    ")\n",
    "print(lasso_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7uEQm0DcvqO"
   },
   "source": [
    "## L1 and L2 regularisation with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMdh4fj1iQa1"
   },
   "source": [
    "So, Lasso regression is simply a regression model that uses L1 regularisation;  a model that uses L2 regularisation is called Ridge regression. This means that we can easily modify our implementation of Linear regression by tweaking the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T13:16:39.357949Z",
     "start_time": "2024-02-24T13:16:39.343209Z"
    },
    "id": "YolUjUfCXufP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, fit loss 50979.28125, reg loss 0.21242070198059082\n",
      "epoch 1, fit loss 5634.40283203125, reg loss 0.19900420308113098\n",
      "epoch 2, fit loss 841.5689086914062, reg loss 0.19751109182834625\n",
      "epoch 3, fit loss 334.422607421875, reg loss 0.19725768268108368\n",
      "epoch 4, fit loss 280.204345703125, reg loss 0.19712890684604645\n",
      "epoch 5, fit loss 273.85467529296875, reg loss 0.19701135158538818\n",
      "epoch 6, fit loss 272.56561279296875, reg loss 0.19689451158046722\n",
      "epoch 7, fit loss 271.8128662109375, reg loss 0.1967778205871582\n",
      "epoch 8, fit loss 271.118408203125, reg loss 0.19666129350662231\n",
      "epoch 9, fit loss 270.43170166015625, reg loss 0.19654501974582672\n",
      "epoch 10, fit loss 269.7474060058594, reg loss 0.19642899930477142\n",
      "epoch 11, fit loss 269.0649108886719, reg loss 0.1963133066892624\n",
      "epoch 12, fit loss 268.3841857910156, reg loss 0.19619786739349365\n",
      "epoch 13, fit loss 267.7052307128906, reg loss 0.1960826963186264\n",
      "epoch 14, fit loss 267.0279541015625, reg loss 0.19596782326698303\n",
      "epoch 15, fit loss 266.35247802734375, reg loss 0.19585324823856354\n",
      "epoch 16, fit loss 265.6787109375, reg loss 0.19573888182640076\n",
      "epoch 17, fit loss 265.0067443847656, reg loss 0.19562488794326782\n",
      "epoch 18, fit loss 264.33642578125, reg loss 0.19551114737987518\n",
      "epoch 19, fit loss 263.6678771972656, reg loss 0.19539764523506165\n",
      "epoch 20, fit loss 263.0009765625, reg loss 0.19528444111347198\n",
      "epoch 21, fit loss 262.3358459472656, reg loss 0.1951715648174286\n",
      "epoch 22, fit loss 261.67242431640625, reg loss 0.1950588971376419\n",
      "epoch 23, fit loss 261.0107116699219, reg loss 0.1949465274810791\n",
      "epoch 24, fit loss 260.3506774902344, reg loss 0.19483445584774017\n",
      "epoch 25, fit loss 259.69232177734375, reg loss 0.19472263753414154\n",
      "epoch 26, fit loss 259.0356750488281, reg loss 0.1946110874414444\n",
      "epoch 27, fit loss 258.3807373046875, reg loss 0.19449982047080994\n",
      "epoch 28, fit loss 257.7275085449219, reg loss 0.19438882172107697\n",
      "epoch 29, fit loss 257.0758972167969, reg loss 0.19427809119224548\n",
      "epoch 30, fit loss 256.42596435546875, reg loss 0.19416764378547668\n",
      "epoch 31, fit loss 255.77774047851562, reg loss 0.19405744969844818\n",
      "epoch 32, fit loss 255.1311798095703, reg loss 0.19394753873348236\n",
      "epoch 33, fit loss 254.4862518310547, reg loss 0.19383788108825684\n",
      "epoch 34, fit loss 253.8429718017578, reg loss 0.193728506565094\n",
      "epoch 35, fit loss 253.2013702392578, reg loss 0.19361937046051025\n",
      "epoch 36, fit loss 252.56137084960938, reg loss 0.1935105323791504\n",
      "epoch 37, fit loss 251.92311096191406, reg loss 0.19340196251869202\n",
      "epoch 38, fit loss 251.28643798828125, reg loss 0.19329363107681274\n",
      "epoch 39, fit loss 250.65139770507812, reg loss 0.19318559765815735\n",
      "epoch 40, fit loss 250.01800537109375, reg loss 0.19307778775691986\n",
      "epoch 41, fit loss 249.38621520996094, reg loss 0.19297029078006744\n",
      "epoch 42, fit loss 248.75607299804688, reg loss 0.19286300241947174\n",
      "epoch 43, fit loss 248.12754821777344, reg loss 0.1927560269832611\n",
      "epoch 44, fit loss 247.5006103515625, reg loss 0.19264927506446838\n",
      "epoch 45, fit loss 246.87530517578125, reg loss 0.19254279136657715\n",
      "epoch 46, fit loss 246.25157165527344, reg loss 0.1924365609884262\n",
      "epoch 47, fit loss 245.6294403076172, reg loss 0.19233062863349915\n",
      "epoch 48, fit loss 245.0089569091797, reg loss 0.1922248899936676\n",
      "epoch 49, fit loss 244.3900604248047, reg loss 0.19211949408054352\n",
      "tensor(260.6404, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Function to compute the L1 loss of all weights in a model\n",
    "def L1regloss(model):\n",
    "    return sum([param.abs().sum() for param in model.parameters()])\n",
    "\n",
    "\n",
    "def L2regloss(model):\n",
    "    return sum([param.pow(2).sum() for param in model.parameters()])\n",
    "\n",
    "\n",
    "model = torch.nn.Linear(8, 1)\n",
    "X_train = torch.from_numpy(np.float32(cali[:15000]))\n",
    "y_train = torch.from_numpy(np.float32(target[:15000])).unsqueeze(1)\n",
    "X_test = torch.from_numpy(np.float32(cali[15000:]))\n",
    "y_test = torch.from_numpy(np.float32(target[15000:])).unsqueeze(1)\n",
    "epochs = 50\n",
    "\n",
    "# Setup optimiser\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-7)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# Main training loop\n",
    "for epoch in range(epochs):\n",
    "    y_predict = model(X_train)\n",
    "    fit_loss = criterion(y_train, y_predict)\n",
    "    # Use L1regloss for Lasso\n",
    "    reg_loss = L2regloss(model)\n",
    "    loss = fit_loss + reg_loss\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(\n",
    "        \"epoch {}, fit loss {}, reg loss {}\".format(\n",
    "            epoch, fit_loss.item(), reg_loss.item()\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Generalisation error\n",
    "print(criterion(model(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNLtKvAni9ZJ"
   },
   "source": [
    "**To do:** \n",
    "\n",
    "Modify the loss function in the above solution to use the L2 loss as penalty instead of L1 loss, thus obtaining an implementation of Ridge regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T11:21:15.468214Z",
     "start_time": "2024-02-23T11:21:15.465045Z"
    },
    "id": "pG7ylmpmjltV"
   },
   "outputs": [],
   "source": [
    "# Your code here - or edit the code above."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5jgbEPaYiwM"
   },
   "source": [
    "# PADL Week 7 Practical including solutions\n",
    "\n",
    "This practical is all about ReLU MLPs. These are still an amazingly useful tool and can be used to solve many real problems (and are still actively used in state-of-the-art research).\n",
    "\n",
    "We'll start by revising the application of an MLP to the breast cancer dataset, as in previous practicals. Then you'll try fitting a ReLU MLP to a given function (a sinusoid) to get an intuition for how they work. Finally, you'll try to classify a highly nonlinear 2D, two class dataset using a ReLU MLP. At the end of each exercise there are some tasks for you to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4-wp-8Wj2gj"
   },
   "source": [
    "## 1. MLP for breast cancer data revisited\n",
    "\n",
    "I've given you the code to load the dataset and train a single hidden layer MLP. Read and understand the code, click through and then try the tasks at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiuXeRVHgO-C"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "from numpy import pi\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhbZvUfwwMAD"
   },
   "source": [
    "Load the dataset and convert to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwMSO04FhlSf"
   },
   "outputs": [],
   "source": [
    "!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
    "\n",
    "training_size = 400\n",
    "\n",
    "data = np.genfromtxt(\"breast-cancer-wisconsin.data\", delimiter=\",\", missing_values=\"?\")\n",
    "data = data[~np.isnan(data).any(axis=1)]\n",
    "\n",
    "# Convert labels to binary 0/1 classes as expected by PyTorch\n",
    "y_01 = np.array([0 if x == 2 else 1 for x in data[:, -1]])\n",
    "\n",
    "X = data[:, 1:-1]  # ignore first column and omit class variable at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilmou7SHxPDT"
   },
   "outputs": [],
   "source": [
    "# Split train/test and convert to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(np.float32(X[:training_size]))\n",
    "Y_train_tensor = torch.from_numpy(np.float32(y_01[:training_size])).unsqueeze(1)\n",
    "X_test_tensor = torch.from_numpy(np.float32(X[training_size:]))\n",
    "Y_test_tensor = torch.from_numpy(np.float32(y_01[training_size:])).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-mKrJpaYyVc"
   },
   "source": [
    "Let's make sure that we understand the structure of our data. Let's start by printing the shapes of the training data inputs and labels (always a good idea!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zK_3aiIScdp1"
   },
   "outputs": [],
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(Y_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6MWjtogY_37"
   },
   "source": [
    "So, our training data comprises 400 samples. Each sample comprises a 9 dimensional input and a binary label. In other words `X_train_tensor[i,:]` is the 9D input for training sample `i` and `Y_train_tensor[i]` contains either a 0 or 1 to indicate which class it belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R13SHu8GxXrN"
   },
   "source": [
    "### Define a single hidden layer MLP\n",
    "\n",
    "We will now define a simple MLP that we will use to solve this problem. This MLP will allow us to specify the input size and the number of neurons in the hidden layer when we instantiate it. It will contain one hidden layer followed by ReLU activation. Since this is a binary classification problem, the output layer will map the output of the hidden layer to a single output which will represent the probability of one of the classes. Since we need the output to represent a probability, we will use sigmoid to map the unconstrained value to a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ECex_lDxDYy"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize):\n",
    "        # Call superclass constructor\n",
    "        super(MLP, self).__init__()\n",
    "        # Initialise components of model:\n",
    "        # 1. First linear layer (hidden layer)\n",
    "        self.linear1 = nn.Linear(inputSize, hiddenSize)\n",
    "        # 2. ReLU layer\n",
    "        self.relu = nn.ReLU()\n",
    "        # 3. Second linear layer (output layer)\n",
    "        self.linear2 = nn.Linear(hiddenSize, 1)\n",
    "        # 4. Sigmoid layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the model:\n",
    "        # x has shape batch x inputSize\n",
    "        # HIDDEN LAYER\n",
    "        # 1. Apply linear layer to input\n",
    "        y = self.linear1(x)\n",
    "        # y has shape batch x 16\n",
    "        # 2. Apply ReLU to output of linear layer\n",
    "        y = self.relu(y)\n",
    "        # OUTPUT LAYER\n",
    "        # 3. Apply linear layer to output of hidden layer\n",
    "        y = self.linear2(y)\n",
    "        # y has shape batch x 1\n",
    "        # 4. Apply sigmoid to output of linear layer\n",
    "        y = self.sigmoid(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBSZDCC9bc-f"
   },
   "source": [
    "Let's just remind ourselves how this model is actually used. First, we want to instantiate it. At this point, the learnable weights and biases are initialised with random values. If you've run some learning and want to start again from scratch, you need to rerun this line, otherwise the weights will be preserved from your previous training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAklPBIKbkM2"
   },
   "outputs": [],
   "source": [
    "# Instantiate MLP with 9 channel input and 16 neuron hidden layer\n",
    "model = MLP(9, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9BkJLBXby2g"
   },
   "source": [
    "Now let's just check that we understand the shapes of inputs and outputs to the model. Let's create some random input and put it through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_w2x2BPbb6Ft"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(100, 9)\n",
    "y = model(x)\n",
    "print(y.shape)\n",
    "print(torch.min(y))\n",
    "print(torch.max(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-ryV3LWb_aU"
   },
   "source": [
    "The first dimension of `x` is the batch size. We are allowed to pass many inputs through a model in parallel and this first dimension (100 in this case) represents this. The second dimension is the size of the input - in this case we have 9D input. The output has shape $100\\times 1$, i.e. a scalar probability for each of the 100 inputs. We can see that the values in `y` lie between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZA3VBcecsmz"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "We're now ready to try training the model. We need to define a loss function, set up the optimiser and then run the training loop. We will use binary cross entropy loss and straightforward gradient descent. For each iteration (=\"epoch\" because we process the entire training set in each operation - this will be different when we come to using minibatches later) of the training loop, we simply put the inputs through our model, get probabilities out (`y_predict`) and put these into the loss function to compare against the binary labels. We then perform backprop and take a gradient descent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UX1aQ50gxK2R"
   },
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "\n",
    "# Instantiate loss function (binary cross entropy loss - sigmoid applied inside model)\n",
    "criterion = torch.nn.BCELoss()\n",
    "# Setup optimiser\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(epochs):\n",
    "    # Pass training data through model\n",
    "    y_predict = model(X_train_tensor)\n",
    "    # Compute BCE loss\n",
    "    loss = criterion(y_predict, Y_train_tensor)\n",
    "    # Backward pass and gradient step\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if not epoch % 200:\n",
    "        # Print out the loss every 10 iterations\n",
    "        print(\"epoch {}, loss {}\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhH1diCriP-B"
   },
   "source": [
    "### Evaluating our model\n",
    "\n",
    "Hopefully, the loss gradually reduced while your model trained. This is promising but the loss value is a bit hard to interpret. Let's now calculate the percentage correct classification on both the training and test sets. To convert the estimated probabilities into hard classifications, we simply threshold against 0.5. We can then compare the predictions against the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hj9cgddmxlHg"
   },
   "outputs": [],
   "source": [
    "# Pass training set set through model\n",
    "y_predict = model(X_train_tensor)\n",
    "# Threshold probabilities to binary classes\n",
    "predictions = (y_predict > 0.5).float()\n",
    "# Compare predicted classes to labels\n",
    "correct = (predictions == Y_train_tensor).float().sum()\n",
    "print(\n",
    "    \"Percent training set correctly classified: {:.2f}%\".format(\n",
    "        100 * correct / training_size\n",
    "    )\n",
    ")\n",
    "\n",
    "# Pass test set through model\n",
    "y_predict = model(X_test_tensor)\n",
    "# Threshold probabilities to binary classes\n",
    "predictions = (y_predict > 0.5).float()\n",
    "# Compare predicted classes to labels\n",
    "correct = (predictions == Y_test_tensor).float().sum()\n",
    "print(\n",
    "    \"Percent test set correctly classified: {:.2f}%\".format(\n",
    "        100 * correct / X_test_tensor.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXrAg2YhlpBk"
   },
   "source": [
    "Depending on your random seed, performance should be in the high 90s and performance on the test set will be slightly different, possibly in either direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMkuqDbcl5iq"
   },
   "source": [
    "### A note of caution: bias\n",
    "\n",
    "When we split the dataset into a training and test set, we simply took the first 400 samples as training data and the remainder as test data. It is always a good idea to check whether your dataset is biased and, if so, whether the bias is the same in both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0V3cAAvJwpVN"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Percent of train set with class 1: {:.2f}\".format(\n",
    "        Y_train_tensor.sum() / Y_train_tensor.shape[0]\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Percent of test set with class 1: {:.2f}\".format(\n",
    "        Y_test_tensor.sum() / Y_test_tensor.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAuwUsi0mwCL"
   },
   "source": [
    "As you can see, our dataset is slightly biased (more class 0 than class 1). However, the bias is worse in the test set than training set. This might be bad.\n",
    "\n",
    "If your goal is to maximise performance for any unseen test set, then ideally the training set would be unbiased. There are various strategies you could use for this. One option is to weight the loss by inverse class frequency. Another is to use a weighted sampling such that samples from the underrepresented class get used in training more often. But we won't worry about this for now.\n",
    "\n",
    "If your goal is to maximise performance on the given test set then we would like the bias to be the same in both training and test set. One strategy to achieve this is to randomly shuffle the data before splitting into training and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvhXf9eHnqJP"
   },
   "source": [
    "### Tasks\n",
    "\n",
    "1. Apply the same random shuffle to `X` and `y_01` before splitting into train and test. Verify that this leads to the same (or very similar) bias in each dataset. Retrain your model. Does this change performance at all?\n",
    "2. Experiment with a smaller MLP - i.e. reduce the number of hidden neurons. How small can you make it while still achieving good performance? If you make it really small, eventually it can't extract any useful information from the features in which case it will only learn the bias (i.e. it will just always predict the more common class and will be right with chance performance). Can you see this happening? What performance do you get?\n",
    "3. Experiment with a deeper/and or wider MLP. i.e. either add additional hidden layers or use more neurons in the hidden layers. Does performance improve? Can you see \"overfitting\" at some point? (Where training loss goes to almost zero but test performance gets worse).\n",
    "4. It's always a good idea to know how many trainable parameters your model has. You should always think about whether your training data is large enough to allow that many parameters to be learnt or if the model is too flexible (in which case it will easily overfit your training data but probably won't generalise). Do some googling to find out how to count the number of trainable parameters in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOjvqzKbyvc_"
   },
   "source": [
    "## 2. Fitting a sinusoid with a ReLU MLP\n",
    "\n",
    "As we've heard in lectures, machine learning is really just *function approximation*. In the next exercise we will literally try to approximate a given function. We will train an MLP to take one input, $x$, and approximately output $\\sin(x)$. Working with a simple 1D function is nice as we can visualise what the network is learning (and you will see how exactly a ReLU MLP approximates a function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJDURaEEyz4c"
   },
   "source": [
    "### Create training data\n",
    "\n",
    "We will uniformly sample 200 values of $x$ from $0$ to $2\\pi$. The labels will simply be $\\sin(x)$ for all these $x$ values. These will form our training data. We can plot $x$ against the labels to see the function we are trying to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKk5x7G4gBFT"
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 2 * pi, 200).unsqueeze(1)\n",
    "label = torch.sin(x)\n",
    "matplotlib.pyplot.plot(x, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rstrpk5Grq-F"
   },
   "source": [
    "### Tasks\n",
    "\n",
    "1. Create a small MLP, `Sin()`, that takes one input, has a single hidden layer with 16 neurons and ReLU activation then an output layer with one output and no activation. This should require only small adaptation from the MLP used in exercise 1.\n",
    "2. Decide on what loss function to use. The task here is regression not classification (you are predicting a continuous value). So don't use binary cross entropy loss.\n",
    "3. Train your model. The training loop will be similar to exercise 1: pass the inputs `x` through your model, get predicted outputs, compare to `label` with your loss function and take a gradient descent step. Keep an eye on the loss value during training. If it's not reducing fast enough, increase learning rate (or try switching to the Adam optimiser). If it's diverging and going crazy, reduce the learning rate.\n",
    "4. Once training has converged, plot the output of your network and compare it to the ground truth sinusoid. How well does it approximate the function? Can you see how ReLU is using straight line segments to approximate a curved function?\n",
    "5. Repeat the above but make your network smaller (e.g. reduce the hidden neurons to 8). How good is the approximation now?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sgbf4caryzqk"
   },
   "source": [
    "## 3. Classifying the spiral dataset\n",
    "\n",
    "The spiral dataset is a good example of a non-trivial binary classification problem. There are two input features for each data sample: the $x$ and $y$ coordinates of a point. The points belong to one of two classes, depending on which of the two interlocking spirals they lie on. The task is to create a small MLP that takes 2D input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YGj4R65Tedk"
   },
   "source": [
    "### Generate the spiral dataset\n",
    "\n",
    "The two classes are points lying on two spirals, rotated by 180 degrees and with some random noise added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HasNGpSm0cS3"
   },
   "outputs": [],
   "source": [
    "N = 400\n",
    "training_size = 600\n",
    "theta = np.sqrt(np.random.rand(N)) * 2 * pi\n",
    "\n",
    "r_a = 2 * theta + pi\n",
    "data_a = np.array([np.cos(theta) * r_a, np.sin(theta) * r_a]).T\n",
    "x_a = data_a + np.random.randn(N, 2)\n",
    "\n",
    "r_b = -2 * theta - pi\n",
    "data_b = np.array([np.cos(theta) * r_b, np.sin(theta) * r_b]).T\n",
    "x_b = data_b + np.random.randn(N, 2)\n",
    "\n",
    "res_a = np.append(x_a, np.zeros((N, 1)), axis=1)\n",
    "res_b = np.append(x_b, np.ones((N, 1)), axis=1)\n",
    "\n",
    "res = np.append(res_a, res_b, axis=0)\n",
    "np.random.shuffle(res)\n",
    "\n",
    "X_train_tensor = torch.from_numpy(np.float32(res[:training_size, 0:2]))\n",
    "Y_train_tensor = torch.from_numpy(np.float32(res[:training_size, 2])).unsqueeze(1)\n",
    "X_test_tensor = torch.from_numpy(np.float32(res[training_size:, 0:2]))\n",
    "Y_test_tensor = torch.from_numpy(np.float32(res[training_size:, 2])).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKwDn4k32o27"
   },
   "source": [
    "Visualise the dataset by plotting the points in the training set and colouring according to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltMZjb0Q0h9f"
   },
   "outputs": [],
   "source": [
    "colors = np.where(Y_train_tensor.squeeze(1).numpy() == 0, \"blue\", \"red\")\n",
    "matplotlib.pyplot.scatter(\n",
    "    X_train_tensor[:, 0].numpy(), X_train_tensor[:, 1].numpy(), c=colors\n",
    ")\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MT6N5T8xdfc"
   },
   "source": [
    "As you can see, it would not be possible linearly separate these two classes (you can't draw a straight line that divides the points into the two classes). However, we will see that a very small ReLU MLP can do this perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymGBAfWR2IIQ"
   },
   "source": [
    "### Tasks\n",
    "\n",
    "1. Instantiate a small classification MLP with 2 inputs, one hidden layer with 16 neurons and sigmoid activation on the output. You can reuse the `MLP()` class from earlier for this.\n",
    "2. Train the MLP to classify the spiral points into two classes.\n",
    "3. Evaluate by printing classification performance on training and test sets. Plot the spiral points for the test set as above, but colour them according to the class predicted by your MLP rather than the ground truth labels.\n",
    "4. Let's visualise what the MLP has actually learnt. For any point in 2D space, the MLP assigns a probability. We can visualise this as an image. Use `torch.meshgrid` to create a grid of points over the range -15..15 in $x$ and $y$. Pass all those points through your model to get probabilities. Display the probabilities as an image using matplotlib's `imshow`. You should be able to see the *decision boundary* i.e. the point where the probability is 0.5 and it is separating the two classes. Can you see the influence of ReLU here?\n",
    "5. As an extra challenge, try to draw the decision boundary over the top of the image as a contour plot. Hint: `skimage.measure.find_contours` will be helpful here.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNbF8iGUhbURX+EXyhLdE5t",
   "provenance": [
    {
     "file_id": "10oE4YoOtY9l_i3DNKmkQSycJamNnEIH6",
     "timestamp": 1712315045249
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
